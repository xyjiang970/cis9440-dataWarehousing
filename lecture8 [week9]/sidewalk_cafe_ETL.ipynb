{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ETL Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, on the top of your Notebook select \"Kernel\" -> \"Restart and Clear Output\"\n",
    "Then, continue from the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup your NYC Open Data variables (ACTION REQUIRED HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sodapy import Socrata\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the host name for the API endpoint (the https:// part will be added automatically)\n",
    "# only need to change this if you are not using NYC Open Data\n",
    "data_url = 'data.cityofnewyork.us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data set at the API endpoint (311 data in this case)\n",
    "# For example: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9.json\n",
    "# would give us 'erm2-nwe9'\n",
    "\n",
    "# https://data.cityofnewyork.us/resource/qcdj-rwhu.json\n",
    "sidewalk_dataset = 'qcdj-rwhu'\n",
    "\n",
    "# https://data.cityofnewyork.us/resource/w7w3-xahh.json\n",
    "businesses_dataset = 'w7w3-xahh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your App Token, which you created in Week 6\n",
    "# You can find your app token by logging into: https://data.cityofnewyork.us/profile/edit/developer_settings\n",
    "app_token = 'your app token here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to setup your Socrata client that connects python to NYC Open Data\n",
    "\n",
    "# create the client that points to the API endpoint\n",
    "nyc_open_data_client = Socrata(data_url, app_token, timeout = 200)\n",
    "print(f\"nyc open data client name is: {nyc_open_data_client}\")\n",
    "print(f\"nyc open data client data type is: {type(nyc_open_data_client)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setup your Google BigQuery variables (ACTION REQUIRED HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not create a key path in class on 3/30/22 (which created a json file on your computer), you must create one to continue:\n",
    "1. Open BigQuery\n",
    "2. On the top-left, click on the Navigation Menu\n",
    "3. In the Navigation Menu, go to \"IAM & Admin\" -> \"Sercive Accounts\"\n",
    "4. On the top of the page, click on \"Create Service Account\"\n",
    "5. Account name: cis9440-spring2022\n",
    "6. Click create and continue\n",
    "7. Set Role to Owner\n",
    "8. Click Continue\n",
    "9. Click Done\n",
    "10. In the new row for your Service Account, click on the 3 dots in the \"Action\" column. Select \"Manage Keys\"\n",
    "11. Click \"Add Key\", then \"Create New Key\". Select the \"JSON\" radio button and click \"Create\"\n",
    "12. In the next cell, set key_path to the exact file path of your new JSON file. For example, it will look like r'C:\\Users\\Downloads\\cis9440-324315-70048a5e1138.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO YOUR FILE PATH\n",
    "key_path = r'your json key path here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell without changing anything to setup your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path,\n",
    "                                                                    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n",
    "bigquery_client = bigquery.Client(credentials = credentials,\n",
    "                                 project = credentials.project_id)\n",
    "\n",
    "print(f\"bigquery client name is: {bigquery_client}\")\n",
    "print(f\"bigquery client data type is: {type(bigquery_client)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to create your dataset id:\n",
    "1. Go to bigquery\n",
    "2. Inside the \"Explorer\" window, click on the 3 dots to the right of your cis9440 project called \"View Actions\"\n",
    "3. Select \"Create dataset\"\n",
    "4. Leave the Project ID as it is, name your Dataset ID etl_dataset\n",
    "5. Expand your cis9440 project with the triangle on its left-hand side so you can see your new etl_dataset dataset\n",
    "6. On the right of your etl_dataset, click the 3 dots for \"View Actions\" -> \"Open\"\n",
    "7. You should now see the \"Dataset info\". Copy the entire \"Dataset ID\" and paste it in the variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'your dataset id here'   # PASTE THIS DATASET ID FROM ABOVE STEPS\n",
    "\n",
    "dataset_id = dataset_id.replace(':', '.')\n",
    "print(f\"your dataset_id is: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. connect to NYC Open Data with API Key\n",
    "2. pull specific dataset as a pandas dataframe\n",
    "3. Look at shape of extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sodapy client.get parameters\n",
    "1. select\n",
    "2. where\n",
    "3. order\n",
    "4. limit\n",
    "5. group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of records in our the entire data set\n",
    "for ds in [sidewalk_dataset, businesses_dataset]:\n",
    "    total_record_count = nyc_open_data_client.get(ds, select = \"COUNT(*)\")\n",
    "    print(f\"total records in {ds}: {total_record_count[0]['COUNT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, loop through target data set to pull all rows in chunks (we cannot pull all rows at once)\n",
    "# AGAIN, UPDATE WHERE FILTER INSIDE BELOW FUNCTION\n",
    "\n",
    "def extract_socrata_data(data_set,\n",
    "                         chunk_size = 2500,\n",
    "                         where = None):\n",
    "    \n",
    "    # measure time this function takes\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # get total number or records\n",
    "    if where == None:\n",
    "        total_records = int(nyc_open_data_client.get(data_set,\n",
    "                                                     select= \"COUNT(*)\")[0][\"COUNT\"])\n",
    "    else:\n",
    "        total_records = int(nyc_open_data_client.get(data_set,\n",
    "                                                     where = where,\n",
    "                                                     select= \"COUNT(*)\")[0][\"COUNT\"])\n",
    "    \n",
    "    # start at 0, empty list for results\n",
    "    start = 0                   \n",
    "    results = []                \n",
    "\n",
    "    while True:\n",
    "\n",
    "        if where == None:\n",
    "            # fetch the set of records starting at 'start'\n",
    "            results.extend(nyc_open_data_client.get(data_set,\n",
    "                                                    offset = start,\n",
    "                                                    limit = chunk_size))\n",
    "            \n",
    "        elif where != None:\n",
    "            results.extend(nyc_open_data_client.get(data_set,\n",
    "                                                    where = where,\n",
    "                                                    offset = start,\n",
    "                                                    limit = chunk_size))\n",
    "        # update the starting record number\n",
    "        start = start + chunk_size\n",
    "\n",
    "        # if we have fetched all of the records (we have reached total_records), exit loop\n",
    "        if (start > total_records):\n",
    "            break\n",
    "\n",
    "    # convert the list into a pandas data frame\n",
    "    data = pd.DataFrame.from_records(results)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"function took {round(end_time - start_time, 1)} seconds\")\n",
    "\n",
    "    print(f\"the shape of your dataframe is: {data.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidewalk_df = extract_socrata_data(chunk_size = 2500,\n",
    "                         data_set = sidewalk_dataset,\n",
    "                         where = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_df = extract_socrata_data(chunk_size = 2500,\n",
    "                         data_set = businesses_dataset,\n",
    "                         where = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidewalk_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sidewalk_df.merge(businesses_df,\n",
    "                         how = 'inner',\n",
    "                         left_on = \"license_nbr\",\n",
    "                         right_on = \"license_nbr\",\n",
    "                         suffixes=('', '_y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distinct values per column\n",
    "2. Null values per column\n",
    "3. Summary statistics per numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the columns in our dataframe?\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for only needed columns\n",
    "data = data[['business_name',\n",
    "             'business_name2',\n",
    "             'latitude',\n",
    "             'longitude',\n",
    "             'street',\n",
    "             'zip',\n",
    "             'address_borough',\n",
    "             'app_status',\n",
    "             'swc_type',\n",
    "             'swc_sq_ft',\n",
    "             'swc_tables',\n",
    "             'swc_chairs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and run a function to ceate data profiling dataframe\n",
    "\n",
    "def create_data_profiling_df(data):\n",
    "    \n",
    "    # create an empty dataframe to gather information about each column\n",
    "    data_profiling_df = pd.DataFrame(columns = [\"column_name\",\n",
    "                                                \"column_type\",\n",
    "                                                \"unique_values\",\n",
    "                                                \"duplicate_values\",\n",
    "                                                \"null_values\",\n",
    "                                                \"non_null_values\"])\n",
    "\n",
    "    # loop through each column to add rows to the data_profiling_df dataframe\n",
    "    for column in data.columns:\n",
    "\n",
    "        info_dict = {}\n",
    "\n",
    "        try:\n",
    "            info_dict[\"column_name\"] = column\n",
    "            info_dict[\"column_type\"] = data[column].dtypes\n",
    "            info_dict[\"unique_values\"] = len(data[column].unique())\n",
    "            info_dict[\"duplicate_values\"] = data[column].count() - len(data[column].dropna().unique())\n",
    "            info_dict[\"null_values\"] = data[column].isna().sum()\n",
    "            info_dict[\"non_null_values\"] = data[column].count()\n",
    "\n",
    "        except:\n",
    "            print(f\"unable to read column: {column}, you may want to drop this column\")\n",
    "\n",
    "        data_profiling_df = data_profiling_df.append(info_dict, ignore_index=True)\n",
    "\n",
    "    data_profiling_df.sort_values(by = ['unique_values', \"non_null_values\"],\n",
    "                                  ascending = [False, False],\n",
    "                                  inplace=True)\n",
    "    \n",
    "    return data_profiling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view your data profiling dataframe\n",
    "data_profiling_df = create_data_profiling_df(data)\n",
    "data_profiling_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. drop unneeded columns\n",
    "2. drop duplicate rows\n",
    "3. check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to look at a list of your columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address null values\n",
    "data['business_name2'].fillna(data['business_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of duplicate rows\n",
    "print(f\"number of duplicate rows: {len(data[data.duplicated()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows based on entire row\n",
    "data = data.drop_duplicates(keep = 'first')\n",
    "\n",
    "# Or, based on a subset of rows, uncomment below and adjust accordingly\n",
    "## data = data.drop_duplicates(subset = [\"subset column\"], keep = 'first')\n",
    "## data = data.drop_duplicates(subset = [\"subset column 1\", \"subset column 2\"], keep = 'first')\n",
    "\n",
    "print(f\"number of rows after duplicates dropped: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update columns types\n",
    "for column in ['latitude',\n",
    "              'longitude',\n",
    "              'zip',\n",
    "              'swc_sq_ft',\n",
    "              'swc_tables',\n",
    "              'swc_chairs']:\n",
    "    try:\n",
    "        data[column] = data[column].astype(int)\n",
    "        \n",
    "    except:\n",
    "        data[column] = data[column].astype(float)\n",
    "        \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Location Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "location_dim = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, subset for only the wanted columns in the dimension\n",
    "location_dim = location_dim[['latitude', 'longitude', 'street',\n",
    "       'zip', 'address_borough']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, drop duplicate rows in dimension\n",
    "unique_row = ['latitude', 'longitude']\n",
    "location_dim = location_dim.drop_duplicates(subset = unique_row, keep = 'first')\n",
    "location_dim = location_dim.reset_index(drop = True)\n",
    "location_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth, add a surrogate key\n",
    "location_dim.insert(0, 'location_id', range(1, 1 + len(location_dim)))\n",
    "location_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifth, add the surrogate key to the data table\n",
    "data = data.merge(location_dim,\n",
    "                  left_on = unique_row,\n",
    "                  right_on = unique_row,\n",
    "                  how = 'left')\n",
    "\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Business Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "business_dim = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_dim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, subset for only the wanted columns in the dimension\n",
    "business_dim = business_dim[['business_name', 'business_name2', 'swc_type', 'app_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, drop duplicate rows in dimension\n",
    "unique_row = ['business_name', 'business_name2']\n",
    "business_dim = business_dim.drop_duplicates(subset = unique_row, keep = 'first')\n",
    "business_dim = business_dim.reset_index(drop = True)\n",
    "business_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth, add a surrogate key\n",
    "business_dim.insert(0, 'business_id', range(1000, 1000 + len(business_dim)))\n",
    "business_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifth, add the surrogate key to the data table\n",
    "data = data.merge(business_dim,\n",
    "                  left_on = unique_row,\n",
    "                  right_on = unique_row,\n",
    "                  how = 'left')\n",
    "\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Creating Fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of fact_table for only the needed columns: which are keys and measures\n",
    "fact_table = data[['business_id',\n",
    "              'location_id',\n",
    "              'swc_sq_ft',\n",
    "               'swc_tables',\n",
    "               'swc_chairs']]\n",
    "\n",
    "fact_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deliver Facts and Dimensions to Data Warehouse (BigQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to load dataframes to BigQuery\n",
    "\n",
    "def load_table_to_bigquery(df,\n",
    "                          table_name,\n",
    "                          dataset_id):\n",
    "\n",
    "    dataset_id = dataset_id #change 301800 to match your project id\n",
    "\n",
    "    dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.autodetect = True\n",
    "    job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "    upload_table_name = f\"{dataset_id}.{table_name}\"\n",
    "    \n",
    "    load_job = bigquery_client.load_table_from_dataframe(df,\n",
    "                                                upload_table_name,\n",
    "                                                job_config = job_config)\n",
    "        \n",
    "    print(f\"\"\"completed loading {table_name} --\n",
    "         {load_job}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_table_to_bigquery(df = location_dim,\n",
    "                       table_name = \"location_dim\",\n",
    "                       dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_table_to_bigquery(df = business_dim,\n",
    "                       table_name = \"business_dim\",\n",
    "                       dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_table_to_bigquery(df = fact_table,\n",
    "                       table_name = \"sidewalk_fact\",\n",
    "                       dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
