{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ETL Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sodapy in c:\\users\\xj438\\appdata\\roaming\\python\\python39\\site-packages (2.2.0)\n",
      "Requirement already satisfied: requests>=2.28.1 in c:\\users\\xj438\\appdata\\roaming\\python\\python39\\site-packages (from sodapy) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.28.1->sodapy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.28.1->sodapy) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.28.1->sodapy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.28.1->sodapy) (2021.10.8)\n",
      "\n",
      "[notice] A new release of pip available: 22.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade sodapy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: db-dtypes in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: pandas<2.0dev,>=0.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from db-dtypes) (1.3.4)\n",
      "Requirement already satisfied: pyarrow<10.0dev,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from db-dtypes) (9.0.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from db-dtypes) (21.0)\n",
      "Requirement already satisfied: numpy<2.0dev,>=1.16.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from db-dtypes) (1.20.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=17.0->db-dtypes) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<2.0dev,>=0.24.2->db-dtypes) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<2.0dev,>=0.24.2->db-dtypes) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas<2.0dev,>=0.24.2->db-dtypes) (1.16.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyarrow) (1.20.3)\n",
      "\n",
      "[notice] A new release of pip available: 22.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: pyarrow<10.0dev,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (9.0.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (1.50.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.10.2)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.16.2)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (21.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (1.22.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (4.21.8)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.4.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\xj438\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-bigquery) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.13.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.56.4)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.50.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from grpcio<2.0dev,>=1.47.0->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-bigquery) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyarrow<10.0dev,>=3.0.0->google-cloud-bigquery) (1.20.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.4.8)\n",
      "\n",
      "[notice] A new release of pip available: 22.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -quests (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -equests (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, on the top of your Notebook select \"Kernel\" -> \"Restart and Clear Output\"\n",
    "Then, continue from the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup your NYC Open Data variables (ACTION REQUIRED HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sodapy import Socrata\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the host name for the API endpoint (the https:// part will be added automatically)\n",
    "# only need to change this if you are not using NYC Open Data\n",
    "data_url = 'data.cityofnewyork.us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data set at the API endpoint (311 data in this case)\n",
    "# For example: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9.json\n",
    "# would give us 'erm2-nwe9'\n",
    "data_set = 't5n6-gx8c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your App Token, which you created in Week 6\n",
    "# You can find your app token by logging into: https://data.cityofnewyork.us/profile/edit/developer_settings\n",
    "#app_token = 'your app token here'\n",
    "app_token = '5NNW7xfYz4R4uazHTapQYNN1l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nyc open data client name is: <sodapy.socrata.Socrata object at 0x0000021B5C89AE20>\n",
      "nyc open data client data type is: <class 'sodapy.socrata.Socrata'>\n"
     ]
    }
   ],
   "source": [
    "# run this cell to setup your Socrata client that connects python to NYC Open Data\n",
    "\n",
    "# create the client that points to the API endpoint\n",
    "nyc_open_data_client = Socrata(data_url, app_token, timeout = 200)\n",
    "print(f\"nyc open data client name is: {nyc_open_data_client}\")\n",
    "print(f\"nyc open data client data type is: {type(nyc_open_data_client)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setup your Google BigQuery variables (ACTION REQUIRED HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not create a key path in class on 3/30/22 (which created a json file on your computer), you must create one to continue:\n",
    "1. Open BigQuery\n",
    "2. On the top-left, click on the Navigation Menu\n",
    "3. In the Navigation Menu, go to \"IAM & Admin\" -> \"Sercive Accounts\"\n",
    "4. On the top of the page, click on \"Create Service Account\"\n",
    "5. Account name: cis9440-spring2022\n",
    "6. Click create and continue\n",
    "7. Set Role to Owner\n",
    "8. Click Continue\n",
    "9. Click Done\n",
    "10. In the new row for your Service Account, click on the 3 dots in the \"Action\" column. Select \"Manage Keys\"\n",
    "11. Click \"Add Key\", then \"Create New Key\". Select the \"JSON\" radio button and click \"Create\"\n",
    "12. In the next cell, set key_path to the exact file path of your new JSON file. For example, it will look like r'C:\\Users\\Downloads\\cis9440-324315-70048a5e1138.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO YOUR FILE PATH\n",
    "key_path = r'your json path here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your json path here'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30320/2918551184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# run this cell without changing anything to setup your credentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m credentials = service_account.Credentials.from_service_account_file(key_path,\n\u001b[0m\u001b[0;32m      3\u001b[0m                                                                     scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n\u001b[0;32m      4\u001b[0m bigquery_client = bigquery.Client(credentials = credentials,\n\u001b[0;32m      5\u001b[0m                                  project = credentials.project_id)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\oauth2\\service_account.py\u001b[0m in \u001b[0;36mfrom_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m                 \u001b[0mcredentials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m         info, signer = _service_account_info.from_filename(\n\u001b[0m\u001b[0;32m    242\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"client_email\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"token_uri\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\auth\\_service_account_info.py\u001b[0m in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require, use_rsa_signer)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0msigner\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \"\"\"\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_rsa_signer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_rsa_signer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your json path here'"
     ]
    }
   ],
   "source": [
    "# run this cell without changing anything to setup your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path,\n",
    "                                                                    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n",
    "bigquery_client = bigquery.Client(credentials = credentials,\n",
    "                                 project = credentials.project_id)\n",
    "\n",
    "print(f\"bigquery client name is: {bigquery_client}\")\n",
    "print(f\"bigquery client data type is: {type(bigquery_client)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to create your dataset id:\n",
    "1. Go to bigquery\n",
    "2. Inside the \"Explorer\" window, click on the 3 dots to the right of your cis9440 project called \"View Actions\"\n",
    "3. Select \"Create dataset\"\n",
    "4. Leave the Project ID as it is, name your Dataset ID etl_dataset\n",
    "5. Expand your cis9440 project with the triangle on its left-hand side so you can see your new etl_dataset dataset\n",
    "6. On the right of your etl_dataset, click the 3 dots for \"View Actions\" -> \"Open\"\n",
    "7. You should now see the \"Dataset info\". Copy the entire \"Dataset ID\" and paste it in the variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'your dataset id here'   # PASTE THIS DATASET ID FROM ABOVE STEPS\n",
    "\n",
    "dataset_id = dataset_id.replace(':', '.')\n",
    "print(f\"your dataset_id is: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. connect to NYC Open Data with API Key\n",
    "2. pull specific dataset as a pandas dataframe\n",
    "3. Look at shape of extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sodapy client.get parameters\n",
    "1. select\n",
    "2. where\n",
    "3. order\n",
    "4. limit\n",
    "5. group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of records in our the entire data set\n",
    "total_record_count = nyc_open_data_client.get(data_set, select = \"COUNT(*)\")\n",
    "print(f\"total records in {data_set}: {total_record_count[0]['COUNT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of records in our target data set\n",
    "# UPDATE YOUR WHERE FILTER HERE IF NEEDED, below is only an example\n",
    "target_record_count = nyc_open_data_client.get(data_set,\n",
    "                                               where = \"Date > '2022-05-01'\",\n",
    "                                               select= \"COUNT(*)\")\n",
    "print(f\"target records in {data_set}: {int(target_record_count[0]['COUNT'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, loop through target data set to pull all rows in chunks (we cannot pull all rows at once)\n",
    "# AGAIN, UPDATE WHERE FILTER INSIDE BELOW FUNCTION\n",
    "\n",
    "def extract_socrata_data(chunk_size = 2500,\n",
    "                         data_set = data_set,\n",
    "                         where = None):\n",
    "    \n",
    "    # measure time this function takes\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # get total number or records\n",
    "    if where == None:\n",
    "        total_records = int(nyc_open_data_client.get(data_set,\n",
    "                                                     select= \"COUNT(*)\")[0][\"COUNT\"])\n",
    "    else:\n",
    "        total_records = int(nyc_open_data_client.get(data_set,\n",
    "                                                     where = where,\n",
    "                                                     select= \"COUNT(*)\")[0][\"COUNT\"])\n",
    "    \n",
    "    # start at 0, empty list for results\n",
    "    start = 0                   \n",
    "    results = []                \n",
    "\n",
    "    while True:\n",
    "\n",
    "        if where == None:\n",
    "            # fetch the set of records starting at 'start'\n",
    "            results.extend(nyc_open_data_client.get(data_set,\n",
    "                                                    offset = start,\n",
    "                                                    limit = chunk_size))\n",
    "            \n",
    "        elif where != None:\n",
    "            results.extend(nyc_open_data_client.get(data_set,\n",
    "                                                    where = where,\n",
    "                                                    offset = start,\n",
    "                                                    limit = chunk_size))\n",
    "        # update the starting record number\n",
    "        start = start + chunk_size\n",
    "\n",
    "        # if we have fetched all of the records (we have reached total_records), exit loop\n",
    "        if (start > total_records):\n",
    "            break\n",
    "\n",
    "    # convert the list into a pandas data frame\n",
    "    data = pd.DataFrame.from_records(results)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"function took {round(end_time - start_time, 1)} seconds\")\n",
    "\n",
    "    print(f\"the shape of your dataframe is: {data.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE DATAFRAME data HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distinct values per column\n",
    "2. Null values per column\n",
    "3. Summary statistics per numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the columns in our dataframe?\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and run a function to ceate data profiling dataframe\n",
    "\n",
    "def create_data_profiling_df(data):\n",
    "    \n",
    "    # create an empty dataframe to gather information about each column\n",
    "    data_profiling_df = pd.DataFrame(columns = [\"column_name\",\n",
    "                                                \"column_type\",\n",
    "                                                \"unique_values\",\n",
    "                                                \"duplicate_values\",\n",
    "                                                \"null_values\",\n",
    "                                                \"non_null_values\"])\n",
    "\n",
    "    # loop through each column to add rows to the data_profiling_df dataframe\n",
    "    for column in data.columns:\n",
    "\n",
    "        info_dict = {}\n",
    "\n",
    "        try:\n",
    "            info_dict[\"column_name\"] = column\n",
    "            info_dict[\"column_type\"] = data[column].dtypes\n",
    "            info_dict[\"unique_values\"] = len(data[column].unique())\n",
    "            info_dict[\"duplicate_values\"] = data[column].count() - len(data[column].dropna().unique())\n",
    "            info_dict[\"null_values\"] = data[column].isna().sum()\n",
    "            info_dict[\"non_null_values\"] = data[column].count()\n",
    "\n",
    "        except:\n",
    "            print(f\"unable to read column: {column}, you may want to drop this column\")\n",
    "\n",
    "        data_profiling_df = data_profiling_df.append(info_dict, ignore_index=True)\n",
    "\n",
    "    data_profiling_df.sort_values(by = ['unique_values', \"non_null_values\"],\n",
    "                                  ascending = [False, False],\n",
    "                                  inplace=True)\n",
    "    \n",
    "    return data_profiling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view your data profiling dataframe\n",
    "RUN DATA PROFILING FUNCTION HERE\n",
    "to create data_profiling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION REQUIRED\n",
    "# If any of the above columns were unable to be read by your function, you may want to drop them\n",
    "# To drop a column, update the column name in the line below and run this cell\n",
    "data.drop([\"column name you would like to drop here\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. drop unneeded columns\n",
    "2. drop duplicate rows\n",
    "3. check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to look at a list of your columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION REQUIRED\n",
    "# edit the drop_columns list below to include all the columns you would like to drop\n",
    "# then, run this cell to drop columns\n",
    "\n",
    "drop_columns = [\"drop columns here\"]\n",
    "\n",
    "for column in drop_columns:\n",
    "    try:\n",
    "        data.drop(column, axis = 1, inplace = True)\n",
    "    except:\n",
    "        print(f\"unable to drop {column}\")\n",
    "\n",
    "print(f\"columns left in dataframe: {data.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of duplicate rows\n",
    "\n",
    "print(f\"number of duplicate rows: {len(data[data.duplicated()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows based on entire row\n",
    "data = data.drop_duplicates(keep = 'first')\n",
    "\n",
    "# Or, based on a subset of rows, uncomment below and adjust accordingly\n",
    "## data = data.drop_duplicates(subset = [\"subset column\"], keep = 'first')\n",
    "## data = data.drop_duplicates(subset = [\"subset column 1\", \"subset column 2\"], keep = 'first')\n",
    "\n",
    "print(f\"number of rows after duplicates dropped: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Location Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "location_dim = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, subset for only the wanted columns in the dimension\n",
    "location_dim = location_dim[[\"route\",\n",
    "                             \"stop\",\n",
    "                             \"direction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, drop duplicate rows in dimension\n",
    "unique_row = [\"unique rows here\"]\n",
    "location_dim = location_dim.drop_duplicates(subset = unique_row, keep = 'first')\n",
    "location_dim = location_dim.reset_index(drop = True)\n",
    "location_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth, add location_id as a surrogate key\n",
    "location_dim.insert(0, 'location_id', range(1, 1 + len(location_dim)))\n",
    "location_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifth, add the location_id to the data table\n",
    "data = data.merge(location_dim,\n",
    "                  left_on = unique_row,\n",
    "                  right_on = unique_row,\n",
    "                  how = 'left')\n",
    "\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Date Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"] = pd.to_datetime(data['date'])\n",
    "data[\"date\"]= data[\"date\"].dt.floor('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACTION REQUIRED: update the start and end date at the bottom of the sql_query variable to fit your needs\n",
    "\n",
    "sql_query = \"\"\"\n",
    "            SELECT\n",
    "              CONCAT (FORMAT_DATE(\"%Y\",d),FORMAT_DATE(\"%m\",d),FORMAT_DATE(\"%d\",d)) as date_id,\n",
    "              d AS full_date,\n",
    "              FORMAT_DATE('%w', d) AS week_day,\n",
    "              FORMAT_DATE('%A', d) AS day_name,\n",
    "              FORMAT_DATE('%B', d) as month_name,\n",
    "              FORMAT_DATE('%Q', d) as fiscal_qtr,\n",
    "              FORMAT_DATE('%Y', d) AS year,\n",
    "            FROM (\n",
    "              SELECT\n",
    "                *\n",
    "              FROM\n",
    "                UNNEST(GENERATE_DATE_ARRAY('2021-01-01', '2023-01-01', INTERVAL 1 DAY)) AS d )\n",
    "            \"\"\"\n",
    "\n",
    "# store extracted data in new dataframe\n",
    "date_dim = bigquery_client.query(sql_query).to_dataframe()\n",
    "\n",
    "# validate that > 0 rows have been extracted and return dataframe\n",
    "if len(date_dim) > 0:\n",
    "    print(f\"date dimension created successfully, shape of dimension: {date_dim.shape}\")\n",
    "else:\n",
    "    print(\"date dimension FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create date_id column in the Fact Table\n",
    "data['date_id'] = data['date'].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"date\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ids = []\n",
    "hours = []\n",
    "minutes = []\n",
    "\n",
    "for hour in range(0,24):\n",
    "    for minute in range(0,60):\n",
    "        time_ids.append((str(hour) + str(minute)).zfill(4))\n",
    "        hours.append(hour)\n",
    "        minutes.append(minute)\n",
    "        \n",
    "time_dim_dict = {\"time_id\" : time_ids,\n",
    "                \"hour\" : hours,\n",
    "                \"minute\" : minutes}\n",
    "\n",
    "time_dim = pd.DataFrame(data = time_dim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE ZFILL AND PAD here to make time_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'hour':'time_id'}, inplace = True)\n",
    "data[\"time_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Creating Fact(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of fact_table for only the needed columns: which are keys and measures\n",
    "SUBSET your fact_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deliver Facts and Dimensions to Data Warehouse (BigQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to load dataframes to BigQuery\n",
    "\n",
    "def load_table_to_bigquery(df,\n",
    "                          table_name,\n",
    "                          dataset_id):\n",
    "\n",
    "    dataset_id = dataset_id #change 301800 to match your project id\n",
    "\n",
    "    dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.autodetect = True\n",
    "    job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "    upload_table_name = f\"{dataset_id}.{table_name}\"\n",
    "    \n",
    "    load_job = bigquery_client.load_table_from_dataframe(df,\n",
    "                                                upload_table_name,\n",
    "                                                job_config = job_config)\n",
    "        \n",
    "    print(f\"completed job {load_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load each of your tables to bigquery"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
